{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('str_list.txt', 'r') as fp:\n",
    "    str_list = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn extracted 821 word features from 13 string.\n"
     ]
    }
   ],
   "source": [
    "# Extracting features from text files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(str_list)\n",
    "print('sklearn extracted %d word features from %d string.'%(X_train_counts.shape[1], X_train_counts.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "['be', 'beams', 'become', 'becomes', 'been', 'before', 'behavior', 'below', 'between', 'beyond', 'binary', 'black', 'blackbody', 'board', 'both', 'brighter', 'brightnesses', 'broad', 'bullet', 'burst']\n"
     ]
    }
   ],
   "source": [
    "print(type(count_vect))\n",
    "#print(count_vect)\n",
    "#print(dir(count_vect))\n",
    "print(count_vect.get_feature_names()[100:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 821)\n"
     ]
    }
   ],
   "source": [
    "data_array_with_features = X_train_counts.toarray() # each column is a feature and each row is a data entry\n",
    "print(data_array_with_features.shape) # ncol=821, nrow=13, number of dimension is 821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=4, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# high dimensional clustering\n",
    "from sklearn import cluster\n",
    "k_means = cluster.KMeans(n_clusters=4)\n",
    "k_means.fit(data_array_with_features) # fit the data array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 2 3 2 1 3 3 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "classified_array = k_means.labels_\n",
    "print(classified_array[::1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(len(X_iris[:,0]))\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "ax1.scatter(X_iris[:,0], X_iris[:,1], c=k_means.labels_)\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax2.scatter(X_iris[:,0], X_iris[:,2], c=k_means.labels_)\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "ax3.scatter(X_iris[:,0], X_iris[:,3], c=k_means.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 821)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "nrow = 13\n",
      "ncol = 821\n",
      "col 62, row 12: [[0.08063341]]\n",
      "col 74, row 12: [[0.11625989]]\n",
      "col 89, row 12: [[0.05812994]]\n",
      "col 150, row 12: [[0.11876939]]\n",
      "col 151, row 12: [[0.20484474]]\n",
      "col 178, row 12: [[0.11876939]]\n",
      "col 210, row 12: [[0.09082398]]\n",
      "col 214, row 12: [[0.11876939]]\n",
      "col 263, row 12: [[0.09082398]]\n",
      "col 296, row 12: [[0.08182756]]\n",
      "col 304, row 12: [[0.10007913]]\n",
      "col 347, row 12: [[0.08182756]]\n",
      "col 363, row 12: [[0.10242237]]\n",
      "col 364, row 12: [[0.11876939]]\n",
      "col 375, row 12: [[0.08063341]]\n",
      "col 446, row 12: [[0.20484474]]\n",
      "col 450, row 12: [[0.20484474]]\n",
      "col 451, row 12: [[0.10242237]]\n",
      "col 455, row 12: [[0.20484474]]\n",
      "col 457, row 12: [[0.10242237]]\n",
      "col 461, row 12: [[0.16365513]]\n",
      "col 463, row 12: [[0.20484474]]\n",
      "col 474, row 12: [[0.23753877]]\n",
      "col 475, row 12: [[0.10242237]]\n",
      "col 522, row 12: [[0.20158352]]\n",
      "col 535, row 12: [[0.10242237]]\n",
      "col 572, row 12: [[0.23753877]]\n",
      "col 581, row 12: [[0.11876939]]\n",
      "col 582, row 12: [[0.11876939]]\n",
      "col 583, row 12: [[0.08182756]]\n",
      "col 595, row 12: [[0.11876939]]\n",
      "col 607, row 12: [[0.23753877]]\n",
      "col 669, row 12: [[0.16365513]]\n",
      "col 684, row 12: [[0.11876939]]\n",
      "col 689, row 12: [[0.10242237]]\n",
      "col 699, row 12: [[0.30726711]]\n",
      "col 731, row 12: [[0.10242237]]\n",
      "col 733, row 12: [[0.11876939]]\n",
      "col 734, row 12: [[0.07447696]]\n",
      "col 736, row 12: [[0.20158352]]\n",
      "col 739, row 12: [[0.10242237]]\n",
      "col 763, row 12: [[0.11876939]]\n",
      "col 768, row 12: [[0.11876939]]\n",
      "col 780, row 12: [[0.11876939]]\n",
      "col 786, row 12: [[0.11876939]]\n",
      "col 798, row 12: [[0.11876939]]\n",
      "col 808, row 12: [[0.11876939]]\n",
      "col 809, row 12: [[0.11876939]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)\n",
    "print(type(X_train_tfidf))\n",
    "print('nrow = %d' % X_train_tfidf.getcol(0).shape[0])\n",
    "print('ncol = %d' % X_train_tfidf.getrow(0).shape[1])\n",
    "j = 12\n",
    "for i in range(X_train_tfidf.getrow(j).shape[1]):\n",
    "    if X_train_tfidf.getrow(j).getcol(i):\n",
    "        #print(X_train_tfidf.getrow(j).getcol(i).shape)\n",
    "        #print(X_train_tfidf.getrow(j).getcol(i).toarray())\n",
    "        print('col %d, row %d: %s' % (i, j, X_train_tfidf.getrow(j).getcol(i).toarray() ) )\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning\n",
    "# Training Naive Bayes (NB) classifier on training data.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
